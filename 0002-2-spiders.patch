From c656e6705e62459d11e34af055cb75f0d7766cad Mon Sep 17 00:00:00 2001
From: xuesu <xuxinyi@ict.ac.cn>
Date: Sun, 7 Jan 2018 11:31:35 +0800
Subject: [PATCH 2/2] 2 spiders

---
 requirements.list         |   7 +-
 spiders/base_spider.py    |   9 ++-
 spiders/sina_spider.py    |  16 +----
 spiders/spider_manager.py |   6 +-
 spiders/tencent_spider.py | 165 ++++++++++++++++++++++++++++++++++++++++++++++
 spiders/toutiao_spider.py | 146 ++++++++++++++++++++++++++++++++++++++++
 test/test_spiders.py      |   4 +-
 utils/utils.py            |  16 +++--
 8 files changed, 344 insertions(+), 25 deletions(-)

diff --git a/requirements.list b/requirements.list
index 6358589..a87b040 100644
--- a/requirements.list
+++ b/requirements.list
@@ -1,8 +1,13 @@
 pyyaml
 pyspark
 Flask
+flask-api
 nose
 pymysql
 sqlalchemy
 simplejson
-jieba
\ No newline at end of file
+jieba
+matplotlib
+pandas
+bs4
+demjson
\ No newline at end of file
diff --git a/spiders/base_spider.py b/spiders/base_spider.py
index b4be3e4..4b77167 100644
--- a/spiders/base_spider.py
+++ b/spiders/base_spider.py
@@ -12,7 +12,14 @@ import requests
 
 class BaseSpider(abc.ABC):
     def __init__(self):
-        self.headers = dict()
+        self.headers = {
+            'Accept': '*/*',
+            'Accept-Encoding': 'gzip, deflate, sdch',
+            'Accept-Language': 'zh-CN,zh;q=0.8',
+            'Cache-Control': 'max-age=0',
+            'Connection': 'keep-alive',
+            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36'
+        }
         self.user_agents = [
             "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36",
             "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36",
diff --git a/spiders/sina_spider.py b/spiders/sina_spider.py
index 74d17f9..772e2c1 100644
--- a/spiders/sina_spider.py
+++ b/spiders/sina_spider.py
@@ -31,16 +31,6 @@ class SinaSpider(spiders.base_spider.BaseSpider):
         # 一页的新闻数
         self.sina_each_page_num = 22
 
-        # header['Host']和Header['Referer']根据需要添加
-        self.headers = {
-            'Accept': '*/*',
-            'Accept-Encoding': 'gzip, deflate, sdch',
-            'Accept-Language': 'zh-CN,zh;q=0.8',
-            'Cache-Control': 'max-age=0',
-            'Connection': 'keep-alive',
-            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36'
-        }
-
     def get_news(self, news_num):
         """
         '获取如下数据：
@@ -69,7 +59,6 @@ class SinaSpider(spiders.base_spider.BaseSpider):
         news_num_per_page = min(self.sina_each_page_num, news_num)
         pages_num = int(news_num / news_num_per_page)
         for i in range(pages_num * 2):
-            logger.info('get %dth page news,each page %d news.' % (i + 1, news_num_per_page))
             page_url = self.sina_news_roll_url.format(news_num_per_page, i + 1)
             try:
                 # 设置headers,读取第i+1页的新闻数据
@@ -78,15 +67,15 @@ class SinaSpider(spiders.base_spider.BaseSpider):
                 # 转为json格式
                 jd = json.loads(page)
             except Exception as e:
-                logger.warning('Failed: ' + page_url)
+                logger.warning('Crawling Roll Failed: {}.'.format(page_url))
                 continue
+            logger.info('Crawling Roll Success: {}.'.format(page_url))
             for news in jd['result']['data']:
                 '''
                 #获取新闻信息：source_id,url,title,keywords,meida_name,abstract,time,news_content,review_num
                 '''
                 # source_id,url,title,keywords,media_name,abstract
                 # time、news_content、review_num到新闻正文页获取
-                news_count += 1
                 # ext2="sh:comos-fynffnz3077632:0"
                 # 提取出comos-fynffnz3077632与相关新闻id格式保持一致
                 news_obj = entities.news.NewsPlain()
@@ -144,6 +133,7 @@ class SinaSpider(spiders.base_spider.BaseSpider):
                     logger.warning("Crawling Review Page Failed: {}".format(review_url))
                 utils.utils.remove_wild_char_in_news(news_obj)
                 datasources.get_db().upsert_news_or_news_list(session, news_obj, commit_now=False)
+                news_count += 1
             datasources.get_db().commit_session(session)
             if news_count >= news_num:
                 break
diff --git a/spiders/spider_manager.py b/spiders/spider_manager.py
index 5a60c95..b2f91f7 100644
--- a/spiders/spider_manager.py
+++ b/spiders/spider_manager.py
@@ -7,6 +7,8 @@ import threading
 
 import my_exceptions.base_exception
 import spiders.sina_spider
+import spiders.tencent_spider
+import spiders.toutiao_spider
 import utils.decorator
 
 
@@ -14,7 +16,9 @@ import utils.decorator
 class SpiderManager(object):
     def __init__(self):
         self.spiders = {
-            "sina": spiders.sina_spider.SinaSpider()
+            "sina": spiders.sina_spider.SinaSpider(),
+            "tencent": spiders.tencent_spider.TencentSpider(),
+            "toutiao": spiders.toutiao_spider.ToutiaoSpider()
         }
 
     def crawl(self, num=None, numbers=None):
diff --git a/spiders/tencent_spider.py b/spiders/tencent_spider.py
index e69de29..a129bbc 100644
--- a/spiders/tencent_spider.py
+++ b/spiders/tencent_spider.py
@@ -0,0 +1,165 @@
+# -*- coding:utf-8 -*-
+"""
+@author: xidongbo
+
+"""
+import bs4
+import datetime
+import simplejson as json
+import itertools
+import random
+
+import datasources
+import entities.news
+import entities.review
+import logs.loggers
+import spiders.base_spider
+import utils.utils
+
+logger = logs.loggers.LoggersHolder().get_logger("spiders")
+
+
+class TencentSpider(spiders.base_spider.BaseSpider):
+    def __init__(self):
+        super(TencentSpider, self).__init__()
+        # 腾讯新闻翻页地址,date=2016-11-29,page=1,2,3……
+        self.tencent_news_roll_url = r'http://roll.news.qq.com/interface/roll.php?%.16f&cata=newssh&site=news&date={}&page={}&mode=1&of=json'
+        # 翻页地址Header.Referer,date=2016-11-29
+        self.tencent_roll_referer = r'http://roll.news.qq.com/index.htm?site=news&mod=1&date={}&cata=newssh'
+        # 一页的新闻数,不能修改
+        self.tencent_each_page_num = 50
+        # 评论翻页地址,review_id,orinum取评论数（最大取100）,orirepnum回复数目，默认2
+        self.tencent_review_roll_url = r'http://coral.qq.com/article/{}/comment/v2?callback=jQuery1124020207774121939615_1511705939893&orinum=100&oriorder=o&pageflag=1&cursor=0&scorecursor=0&orirepnum=2&reporder=o&reppageflag=1&source=1&_=1511705939894'
+        self.sina_each_page_num = 50
+
+    def get_news(self, news_num):
+        """
+        '获取如下数据：
+            '获取新闻数据：
+                source_id:新闻id,
+                url:新闻链接,
+                title:新闻标题,
+                keywords:新闻关键词,
+                media_name:发布媒体名称,
+                abstract:新闻摘要,
+                time:发布时间,
+                news_content:新闻内容,
+                review_num:评论条数
+            '相关新闻数据:related_id:相关的新闻id列表
+            '评论数据:
+                user_id:用户id,
+                user_name:用户昵称,
+                area:评论地点,
+                review_content:评论内容,
+                time:评论时间,
+                agree:点赞数
+        """
+        session = datasources.get_db().create_session()
+        news_count = 0
+        day_num = (news_num + self.tencent_each_page_num - 1) // self.tencent_each_page_num * 2
+        for delta_day in range(day_num):
+            date = utils.utils.get_date_before(delta_day)
+            # 每天最多三页新闻：3*50
+            for page_index in range(1):
+                page_url = (self.tencent_news_roll_url % random.random()).format(date, page_index + 1)
+                try:
+                    # 设置headers,读取第i+1页的新闻数据
+                    page = self.get_response(referer=self.tencent_roll_referer.format(date),
+                                             url=page_url, encoding='gbk')
+                    # 转为json格式
+                    jd = json.loads(page)
+                except Exception as e:
+                    logger.warning('Crawling Roll Failed: {}.'.format(page_url))
+                    continue
+                if str(jd['response']['code']) != '0':
+                    break
+                # 得到一页html格式的50条新闻
+                page_html = jd['data']['article_info']
+                try:
+                    page_bs = bs4.BeautifulSoup(page_html, 'html.parser')
+                except Exception as e:
+                    logger.warning('Crawling Roll Failed: {}.'.format(page_url))
+                    continue
+                # ‘#’查找id名，‘.’查找class名 , 直接查找标签名
+                for news in page_bs.select('li'):
+                    '''
+                    #获取新闻信息：url,title,time
+                    '''
+                    news_obj = entities.news.NewsPlain()
+                    timestr = date[:4] + '-' + news.span.text
+                    if timestr.index(' ') == len(timestr) - 5:
+                        timestr = timestr[:timestr.index(' ') + 1] + '0' + timestr[timestr.index(' ') + 1:]
+                    try:
+                        news_obj.time = datetime.datetime.strptime(timestr, "%Y-%m-%d %H:%M")
+                    except Exception as e:
+                        logger.warning('Crawling Roll Failed: {}.'.format(page_url))
+                        continue
+                    news_obj.url = news.a['href']
+                    news_obj.title = news.a.text
+                    news_obj.source = news_obj.SourceEnum.tencent
+                    try:
+                        # 设置headers
+                        # 获取新闻正文页html
+                        news_html = self.get_response(referer='', url=news_obj.url, encoding='gbk')
+                        soup = bs4.BeautifulSoup(news_html, 'html.parser')
+                        # ‘#’查找id名，‘.’查找class名
+                        news_obj.media_name = soup.select('.a_source')[0].text
+                        news_obj.content = '\n'.join([p.text for p in soup.select('#Cnt-Main-Article-QQ')[0].select('p')])
+                        news_obj.abstract = news_obj.content[:100]
+                        review_info = str(soup)
+                        idx = review_info.index('cmt_id')
+                        review_info = review_info[idx + 9:idx + 19]
+                        # review_info just like 'cmt_id= 111111;'
+                        review_page_id = review_info.strip()
+                        new_info = str(soup.html.head.find_next(name='script', attrs={'type': 'text/javascript'}))
+                        id_str = new_info.split('\n')[6]
+                        news_obj.source_id = id_str[id_str.index('\'') + 1:id_str.rindex('\'')]
+                        # keywords_str just like tags:['a','b','c'],
+                        keywords_str = new_info.split('\n')[11]
+                        news_obj.keywords = keywords_str[keywords_str.index('[') + 1:keywords_str.rindex(']')]
+                        news_obj.keywords = news_obj.keywords.replace('\'', '')
+                    except Exception as e:
+                        # set default
+                        logger.warning("Crawling Content Failed: {}".format(news_obj.url))
+                        continue
+                    logger.info("Crawling Content Success: {}".format(news_obj.url))
+                    # 若爬取评论失败，则为0
+                    news_obj.review_num = 0
+                    if review_page_id:
+                        try:
+                            review_url = self.tencent_review_roll_url.format(review_page_id)
+                            logger.info("Crawling Review Page Success: {}".format(review_url))
+                        except Exception as e:
+                            logger.warning("Crawling Review Page Failed: {}".format(review_url))
+                        try:
+                            review_page = self.get_response(
+                                referer='http://page.coral.qq.com/coralpage/comment/news.html',
+                                url=review_url, encoding='gbk')
+                            jd = json.loads(review_page[review_page.index('{'):review_page.rindex('}') + 1])
+                            news_obj.review_num = jd['data']['oritotal']
+                            # 原始评论
+                            comm_list = itertools.chain(jd['data']['oriCommList'], *jd['data']['repCommList'])
+                            for review in comm_list:
+                                review_obj = entities.review.ReviewPlain()
+                                review_obj.user_id = review['userid']
+                                review_obj.content = review['content']
+                                seconds = int(review['time'])
+                                review_obj.time = datetime.datetime.fromtimestamp(seconds)
+                                review_obj.agree = int(review['up'])
+                                user_info = jd['data']['userList'][review_obj.user_id]
+                                review_obj.user_name = user_info['nick']
+                                review_obj.area = user_info['region']
+                                if review_obj.area is not None:
+                                    review_obj.area = review_obj.area[-19:]
+                                news_obj.reviews.append(review_obj)
+                        except Exception as e:  # 评论出错直接忽略
+                            logger.warning("Handling an invalid comment Failed.")
+                    utils.utils.remove_wild_char_in_news(news_obj)
+                    datasources.get_db().upsert_news_or_news_list(session, news_obj, commit_now=False)
+                    news_count += 1
+                datasources.get_db().commit_session(session)
+                if news_count >= news_num:
+                    break
+            if news_count >= news_num:
+                break
+        datasources.get_db().close_session(session)
diff --git a/spiders/toutiao_spider.py b/spiders/toutiao_spider.py
index e69de29..4e7df62 100644
--- a/spiders/toutiao_spider.py
+++ b/spiders/toutiao_spider.py
@@ -0,0 +1,146 @@
+# -*- coding:utf-8 -*-
+"""
+@author: xidongbo
+
+"""
+import bs4
+import datetime
+import demjson
+import simplejson as json
+import random
+import time
+
+import datasources
+import entities.news
+import entities.review
+import logs.loggers
+import spiders.base_spider
+import utils.utils
+
+logger = logs.loggers.LoggersHolder().get_logger("spiders")
+
+
+class ToutiaoSpider(spiders.base_spider.BaseSpider):
+    def __init__(self):
+        super(ToutiaoSpider, self).__init__()
+        self.toutiao_news_roll_url = r'https://www.toutiao.com/api/pc/feed/?category=news_society&utm_source=toutiao&widen=1&max_behot_time={}&max_behot_time_tmp={}&tadrequire=true&as=A125BA93E6A7F96&cp=5A3697EF09466E1&_signature=kuBu4QAAyOGy.bX4veRHx5Lgbv'
+        # group_id={}&item_id={}
+        self.toutiao_review_roll_url = r'https://www.toutiao.com/api/comment/list/?group_id={}&item_id={}&offset=0&count=20'
+        self.toutiao_num = 30000
+        self.toutiao_each_page_num = 7
+
+    def get_news(self, news_num):
+        """
+        '获取如下数据：
+            '获取新闻数据：
+                source_id:新闻id,
+                url:新闻链接,
+                title:新闻标题,
+                keywords:新闻关键词,
+                media_name:发布媒体名称,
+                abstract:新闻摘要,
+                time:发布时间,
+                news_content:新闻内容,
+                review_num:评论条数
+            '相关新闻数据:related_id:相关的新闻id列表
+            '评论数据:
+                user_id:用户id,
+                user_name:用户昵称,
+                area:评论地点,
+                review_content:评论内容,
+                time:评论时间,
+                agree:点赞数
+        """
+        session = datasources.get_db().create_session()
+        news_count = 0
+        now_time = int(time.time())
+        # 一共要爬取的页数
+        news_num_per_page = min(self.toutiao_each_page_num, news_num)
+        pages_num = int(news_num / news_num_per_page)
+        for i in range(pages_num * 2):
+            page_url = self.toutiao_news_roll_url.format(now_time, now_time)
+            try:
+                # 设置headers,读取第i+1页的新闻数据
+                page = self.get_response('https://www.toutiao.com/ch/news_society/', page_url)
+                page = page[page.index('{'):page.rindex('}') + 1]
+                # 转为json格式
+                jd = json.loads(page)
+                now_time = jd['next']['max_behot_time']
+            except Exception as e:
+                logger.warning('Crawling Roll Failed: {}.'.format(page_url))
+                now_time -= random.randint(0, 3600)
+                continue
+            logger.info('Crawling Roll Success: {}.'.format(page_url))
+            for news in jd['data']:
+                '''
+                #获取新闻信息：source_id,url,title,keywords,meida_name,abstract,time,news_content,review_num
+                '''
+                # source_id,url,title,keywords,media_name,abstract
+                # time、news_content、review_num到新闻正文页获取
+                # ext2="sh:comos-fynffnz3077632:0"
+                # 提取出comos-fynffnz3077632与相关新闻id格式保持一致
+                if news['is_feed_ad']:
+                    continue
+                news_obj = entities.news.NewsPlain()
+                try:
+                    news_obj.source_id = 'a'+news['group_id']
+                    if datasources.get_db().find_news_by_source_id(session, source_id=news_obj.source_id):
+                        continue
+                    news_obj.url = 'https://www.toutiao.com/' + news_obj.source_id
+                    news_obj.title = news['title']
+                    news_obj.keywords = ','.join(news['label'])
+                    news_obj.media_name = news['source']
+                    if news_obj.media_name == '悟空问答':
+                        continue
+                    news_obj.abstract = news['abstract']
+                    news_obj.review_num = int(news['comments_count'])
+                    news_obj.source = news_obj.SourceEnum.toutiao
+
+                    # 设置headers
+                    # 获取新闻正文页html,提取news_content
+                    news_html = self.get_response(page_url, news_obj.url)
+                    news_html = news_html[news_html.index('articleInfo:'):news_html.rindex('commentInfo')]
+                    news_html = news_html[news_html.index('{'):news_html.rindex('}') + 1]
+                    dj = demjson.decode(news_html)
+                    soup = bs4.BeautifulSoup(dj['content'], 'html.parser').text
+                    soup = bs4.BeautifulSoup(soup, 'html.parser')
+                    # ‘#’查找id名，‘.’查找class名
+                    news_obj.time = datetime.datetime.strptime(dj['subInfo']['time'], "%Y-%m-%d %H:%M:%S")
+                    news_obj.content = '\n'.join([p.text for p in soup.select('p')])
+                    group_id = dj['groupId']
+                    item_id = dj['itemId']
+                    logger.info("Crawling Content Success: {}".format(news_obj.url))
+                except Exception as e:
+                    logger.warning("Crawling Content Failed: {}".format(news_obj.url))
+                    continue
+
+                review_url = self.toutiao_review_roll_url.format(group_id, item_id)
+                try:
+                    '''
+                    #获取评论信息：user_id,user_name,area,review_content,time,agree
+                    '''
+                    # self.review_num是真实的评论数量，可作为热度的参考值。
+                    # 但是评论内容最多取100条
+                    review_page = self.get_response('https://www.toutiao.com/a{}/'.format(group_id), review_url)
+                    jd = json.loads(review_page)
+                    news_obj.review_num = jd['data']['total']
+                    for review in jd['data']['comments']:
+                        review_obj = entities.review.ReviewPlain()
+                        review_obj.user_id = review['user']['user_id']
+                        review_obj.user_name = review['user']['name']
+                        review_obj.content = review['text']
+                        seconds = float(review['create_time'])
+                        review_obj.time = datetime.datetime.fromtimestamp(seconds)
+                        review_obj.agree = review['digg_count']
+                        news_obj.reviews.append(review_obj)
+                    logger.info("Crawling Review Page Success: {}".format(review_url))
+                except Exception as e:
+                    # 评论出错直接忽略
+                    logger.warning("Crawling Review Page Failed: {}".format(review_url))
+                utils.utils.remove_wild_char_in_news(news_obj)
+                datasources.get_db().upsert_news_or_news_list(session, news_obj, commit_now=False)
+                news_count += 1
+            datasources.get_db().commit_session(session)
+            if news_count >= news_num:
+                break
+        datasources.get_db().close_session(session)
diff --git a/test/test_spiders.py b/test/test_spiders.py
index e7ca40a..513881f 100644
--- a/test/test_spiders.py
+++ b/test/test_spiders.py
@@ -25,8 +25,6 @@ class SpiderManagerTest(test.TestCase):
 
     def test_crawl(self):
         db = datasources.get_db()
-        db.recreate_all_tables()
         session = db.create_session()
         self.spider_manager.crawl(num=4)
-        self.assertIsNotNone(db.find_news_list(session))
-        db.recreate_all_tables()
+        self.assertIsNot(len(db.find_news_list(session)), 0)
diff --git a/utils/utils.py b/utils/utils.py
index 9a5ebee..bf1bc5c 100644
--- a/utils/utils.py
+++ b/utils/utils.py
@@ -14,16 +14,18 @@ import re
 def get_date_before(shift=0):
     # 返回当前日期shift天后的日期，正数代表将来，负数代表过去，格式：‘2017-11-15’
     now_time = datetime.datetime.now()
-    date = now_time + datetime.timedelta(days=shift)
+    date = now_time - datetime.timedelta(days=shift)
     return date.strftime('%Y-%m-%d')
 
 
 def remove_wild_char(s):
-    s = re.sub(r"[^\s\w`=\\;.!/_,$^*+\"\'\[\]—！，。？、~@#￥%…&（）：；《》“”()»〔〕?\-]", "", s)
-    s = re.sub("\r", "\n", s)
-    s = re.sub("\n\n", "\n", s)
-    s = re.sub("\n +", "\n", s)
-    return s.strip()
+    if s is not None:
+        s = re.sub(r"[^\s\w`=\\;.!/_,$^*+\"\'\[\]—！，。？、~@#￥%…&（）：；《》“”()»〔〕?\-]", "", s)
+        s = re.sub("\r", "\n", s)
+        s = re.sub("\n\n", "\n", s)
+        s = re.sub("\n +", "\n", s)
+        return s.strip()
+    return ''
 
 
 def remove_wild_char_in_news(news):
@@ -37,6 +39,8 @@ def remove_wild_char_in_news(news):
 
 def remove_wild_char_in_review(review):
     review.content = remove_wild_char(review.content)
+    review.user_name = remove_wild_char(review.user_name)
+    review.area = remove_wild_char(review.area)
 
 
 def remove_new_line(s):
-- 
2.14.2

